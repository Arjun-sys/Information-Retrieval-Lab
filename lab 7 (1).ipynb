{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Data\n",
      "  team  points\n",
      "0    A      25\n",
      "1    B      15\n",
      "2    B      14\n",
      "3    C      19\n",
      "\n",
      "One Hot Encoded Data\n",
      "   points  teamA  teamB  teamC\n",
      "0      25    1.0    0.0    0.0\n",
      "1      15    0.0    1.0    0.0\n",
      "2      14    0.0    1.0    0.0\n",
      "3      19    0.0    0.0    1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#create DataFrame\n",
    "df = pd.DataFrame({'team': ['A','B', 'B', 'C'],\n",
    "                   'points': [25,15, 14, 19]})\n",
    "\n",
    "#view DataFrame\n",
    "print(\"Orignal Data\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#perform one-hot encoding on 'team' column \n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df[['team']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df.join(encoder_df)\n",
    "\n",
    "#view final df\n",
    "# print(final_df)\n",
    "#drop 'team' column\n",
    "final_df.drop('team', axis=1, inplace=True)\n",
    "\n",
    "#view final df\n",
    "# print(final_df)\n",
    "#rename columns\n",
    "final_df.columns = ['points', 'teamA', 'teamB', 'teamC']\n",
    "\n",
    "#view final df\n",
    "print(\"\\nOne Hot Encoded Data\")\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'one': 6, 'man': 4, 'has': 2, 'two': 7, 'hands': 1, 'men': 5, 'have': 3, 'four': 0}\n",
      "Encoded Document is:\n",
      "[[0 1 1 0 1 0 1 1]\n",
      " [1 1 0 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "  \n",
    "document = [\"One man has two hands\",\n",
    "            \"Two men have four hands\"]\n",
    "  \n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "  \n",
    "vectorizer.fit(document)\n",
    "  \n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "  \n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(document)\n",
    "  \n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_ngrams(text,n):\n",
    "    # split sentences into tokens\n",
    "    tokens=re.split(\"\\\\s+\",text)\n",
    "    ngrams=[]\n",
    "    # collect the n-grams\n",
    "    for i in range(len(tokens)-n+1):\n",
    "       temp=[tokens[j] for j in range(i,i+n)]\n",
    "       ngrams.append(\" \".join(temp))\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'cow', 'jumps')\n",
      "('cow', 'jumps', 'over')\n",
      "('jumps', 'over', 'the')\n",
      "('over', 'the', 'moon')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'the cow jumps over the moon'\n",
    "my_grams = ngrams(sentence.split(), 3)\n",
    "\n",
    "for grams in my_grams:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "for : 1.6931471805599454\n",
      "geeks : 1.2876820724517808\n",
      "help : 1.6931471805599454\n",
      "\n",
      "Word indexes:\n",
      "{'geeks': 1, 'for': 0, 'help': 2}\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.54935123 0.83559154 0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  \n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'help'\n",
    "  \n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]\n",
    "  \n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "  \n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "  \n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)\n",
    "  \n",
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "  \n",
    "# display tf-idf values\n",
    "# print('\\ntf-idf value:')\n",
    "# print(result)\n",
    "  \n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
